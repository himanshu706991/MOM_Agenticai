{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28af3cb1",
   "metadata": {},
   "source": [
    "# Multi-Agent MoM Generator — with Final GPT Polish (LangGraph + LangChain)\n",
    "\n",
    "This notebook extends the pipeline by adding a **final GPT pass** after the four agents:\n",
    "**Extract → Summarize → Critique → Aggregate → GPT Polish**\n",
    "\n",
    "Configure via environment variables; supports **Ollama / HF / OpenAI** for the main agents and a separate GPT polish step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388d96ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional: install (uncomment)\n",
    "# %pip install langchain langgraph pydantic python-docx pymupdf reportlab transformers accelerate torch ollama-client openai tiktoken python-dotenv ipywidgets\n",
    "\n",
    "import os, json, re\n",
    "from typing import Dict, Any\n",
    "\n",
    "# ------------------ Config ------------------\n",
    "LLM_BACKEND = os.getenv(\"LLM_BACKEND\", \"ollama\")  # ollama | hf | openai\n",
    "OLLAMA_MODEL = os.getenv(\"OLLAMA_MODEL\", \"llama3\")\n",
    "HF_MODEL = os.getenv(\"HF_MODEL\", \"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "\n",
    "LLM_FINAL_BACKEND = os.getenv(\"LLM_FINAL_BACKEND\", \"openai\")  # openai | ollama | hf | same\n",
    "FINAL_MODEL = os.getenv(\"FINAL_MODEL\", \"gpt-4o-mini\")\n",
    "\n",
    "def try_json_loads(s: str) -> Any:\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        s2 = re.sub(r\"```json|```\", \"\", s).strip()\n",
    "        s2 = re.sub(r\",\\s*([}\\]])\", r\"\\1\", s2)\n",
    "        try:\n",
    "            return json.loads(s2)\n",
    "        except Exception:\n",
    "            return {\"raw\": s}\n",
    "\n",
    "# ------------------ LLM Adapters ------------------\n",
    "class LLMAdapter:\n",
    "    def __init__(self, backend: str, model_overrides: Dict[str,str]|None=None):\n",
    "        self.backend = backend\n",
    "        self.model_overrides = model_overrides or {}\n",
    "        if backend == \"ollama\":\n",
    "            from ollama import Client as OllamaClient  # type: ignore\n",
    "            self.client = OllamaClient(host=os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\"))\n",
    "        elif backend == \"hf\":\n",
    "            from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline  # type: ignore\n",
    "            model_name = self.model_overrides.get(\"hf\", os.getenv(\"HF_MODEL\", \"mistralai/Mistral-7B-Instruct-v0.2\"))\n",
    "            tok = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\")\n",
    "            self.pipe = pipeline(\"text-generation\", model=model, tokenizer=tok, max_new_tokens=1024)\n",
    "        elif backend == \"openai\":\n",
    "            from openai import OpenAI  # type: ignore\n",
    "            self.client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\", \"\"))\n",
    "        elif backend == \"same\":\n",
    "            self.client = None\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown backend: {backend}\")\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        if self.backend == \"ollama\":\n",
    "            model = self.model_overrides.get(\"ollama\", os.getenv(\"OLLAMA_MODEL\", \"llama3\"))\n",
    "            out = self.client.generate(model=model, prompt=prompt, stream=False)\n",
    "            return out.get(\"response\", \"\").strip()\n",
    "        elif self.backend == \"hf\":\n",
    "            gen = self.pipe(prompt)[0][\"generated_text\"]\n",
    "            return gen[len(prompt):].strip() if gen.startswith(prompt) else gen\n",
    "        elif self.backend == \"openai\":\n",
    "            model = self.model_overrides.get(\"openai\", os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\"))\n",
    "            chat = self.client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\":\"system\",\"content\":\"You are a precise, structured writing assistant.\"},\n",
    "                    {\"role\":\"user\",\"content\":prompt}\n",
    "                ],\n",
    "                temperature=0.2\n",
    "            )\n",
    "            return chat.choices[0].message.content\n",
    "        elif self.backend == \"same\":\n",
    "            raise RuntimeError(\"Backend 'same' is a placeholder; set a real adapter.\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported backend\")\n",
    "\n",
    "# ------------------ Ingestion ------------------\n",
    "import fitz  # PyMuPDF\n",
    "from docx import Document\n",
    "from docx.shared import Pt\n",
    "from reportlab.lib.pagesizes import A4\n",
    "from reportlab.pdfgen import canvas\n",
    "\n",
    "def extract_text_from_pdf(path: str) -> str:\n",
    "    text = []\n",
    "    with fitz.open(path) as doc:\n",
    "        for page in doc:\n",
    "            text.append(page.get_text())\n",
    "    return \"\\n\".join(text).strip()\n",
    "\n",
    "def extract_text_from_docx(path: str) -> str:\n",
    "    doc = Document(path)\n",
    "    return \"\\n\".join(p.text for p in doc.paragraphs)\n",
    "\n",
    "def extract_text_from_txt(path: str) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def ingest_any(path: str) -> str:\n",
    "    p = path.lower()\n",
    "    if p.endswith(\".pdf\"):\n",
    "        return extract_text_from_pdf(path)\n",
    "    if p.endswith(\".docx\"):\n",
    "        return extract_text_from_docx(path)\n",
    "    if p.endswith(\".txt\"):\n",
    "        return extract_text_from_txt(path)\n",
    "    raise ValueError(\"Unsupported file type. Use PDF, DOCX, or TXT.\")\n",
    "\n",
    "# ------------------ Prompts ------------------\n",
    "EXTRACT_PROMPT = \"\"\"\n",
    "You are an information extraction agent for meeting transcripts.\n",
    "From the text below, return strict JSON with keys:\n",
    "- participants: [ { \"name\": \"...\", \"role\": \"...\" } ]\n",
    "- meeting_title: string\n",
    "- meeting_datetime: string (ISO or as stated)\n",
    "- agenda: [string]\n",
    "- key_points: [string]\n",
    "- decisions: [string]\n",
    "- risks: [string]\n",
    "- action_items: [ { \"description\": \"...\", \"owner\": \"...\", \"due_date\": \"...\", \"priority\": \"High/Med/Low\" } ]\n",
    "\n",
    "Return only JSON.\n",
    "TEXT:\n",
    "{transcript}\n",
    "\"\"\"\n",
    "\n",
    "SUMMARY_PROMPT = \"\"\"\n",
    "You are a summarization agent. Convert this extracted JSON into a\n",
    "clear, detailed Minutes of Meeting (MoM) with sections:\n",
    "- Meeting Title\n",
    "- Date/Time\n",
    "- Participants (Name - Role)\n",
    "- Agenda\n",
    "- Summary (bulleted, but rich and specific)\n",
    "- Decisions\n",
    "- Action Items (Owner - Description - Due Date - Priority)\n",
    "- Risks / Open Questions\n",
    "- Next Steps (with owners and target dates)\n",
    "\n",
    "Keep it client-ready, concise but comprehensive.\n",
    "EXTRACTED_JSON:\n",
    "{extracted_json}\n",
    "\"\"\"\n",
    "\n",
    "CRITIQUE_PROMPT = \"\"\"\n",
    "You are a critical reviewer. Read the MoM draft and identify missing details,\n",
    "ambiguities, tone/style issues, and any inconsistencies with typical MoM standards.\n",
    "Return JSON with keys:\n",
    "- missing_info: [string]\n",
    "- suggested_edits: [string]\n",
    "- tone_issues: [string]\n",
    "- formatting_changes: [string]\n",
    "\n",
    "Return only JSON.\n",
    "MOM_DRAFT:\n",
    "{mom_draft}\n",
    "\"\"\"\n",
    "\n",
    "AGGREGATE_PROMPT = \"\"\"\n",
    "You are a senior editor. Merge the critique into the MoM draft and produce\n",
    "the final detailed, polished MoM with the same sections and consistent formatting.\n",
    "Apply suggested edits and fill reasonable gaps if evidence exists in the transcript.\n",
    "Return final MoM as plain text (no JSON).\n",
    "\n",
    "MOM_DRAFT:\n",
    "{mom_draft}\n",
    "\n",
    "CRITIQUE_JSON:\n",
    "{critique_json}\n",
    "\"\"\"\n",
    "\n",
    "FINAL_GPT_PROMPT = \"\"\"\n",
    "You are a senior communications editor. Lightly polish the following Minutes of Meeting (MoM) for clarity,\n",
    "professional tone, grammar, and formatting without changing factual content. Keep the same sections and structure.\n",
    "Return the final MoM as plain text.\n",
    "\n",
    "MOM_FINAL_DRAFT:\n",
    "{mom_final_draft}\n",
    "\"\"\"\n",
    "\n",
    "# ------------------ Agents ------------------\n",
    "llm_main = LLMAdapter(LLM_BACKEND)\n",
    "\n",
    "def extract_agent(transcript: str) -> Dict[str, Any]:\n",
    "    prompt = EXTRACT_PROMPT.format(transcript=transcript[:150000])\n",
    "    data = try_json_loads(llm_main.generate(prompt))\n",
    "    return {\n",
    "        \"participants\": data.get(\"participants\", []),\n",
    "        \"meeting_title\": data.get(\"meeting_title\", \"\"),\n",
    "        \"meeting_datetime\": data.get(\"meeting_datetime\", \"\"),\n",
    "        \"agenda\": data.get(\"agenda\", []),\n",
    "        \"key_points\": data.get(\"key_points\", []),\n",
    "        \"decisions\": data.get(\"decisions\", []),\n",
    "        \"risks\": data.get(\"risks\", []),\n",
    "        \"action_items\": data.get(\"action_items\", []),\n",
    "    }\n",
    "\n",
    "def summarize_agent(extracted: Dict[str, Any]) -> str:\n",
    "    prompt = SUMMARY_PROMPT.format(extracted_json=json.dumps(extracted, ensure_ascii=False))\n",
    "    return llm_main.generate(prompt)\n",
    "\n",
    "def critique_agent(mom_draft: str) -> Dict[str, Any]:\n",
    "    prompt = CRITIQUE_PROMPT.format(mom_draft=mom_draft)\n",
    "    return try_json_loads(llm_main.generate(prompt))\n",
    "\n",
    "def aggregate_agent(mom_draft: str, critique: Dict[str, Any]) -> str:\n",
    "    prompt = AGGREGATE_PROMPT.format(mom_draft=mom_draft, critique_json=json.dumps(critique, ensure_ascii=False))\n",
    "    return llm_main.generate(prompt)\n",
    "\n",
    "def final_gpt_polish(mom_final_draft: str) -> str:\n",
    "    backend = LLM_FINAL_BACKEND\n",
    "    if backend == \"openai\" and not os.getenv(\"OPENAI_API_KEY\", \"\"):\n",
    "        backend = LLM_BACKEND\n",
    "    if backend == \"same\":\n",
    "        backend = LLM_BACKEND\n",
    "    llm_final = LLMAdapter(backend, model_overrides={\n",
    "        \"openai\": os.getenv(\"FINAL_MODEL\", \"gpt-4o-mini\"),\n",
    "        \"ollama\": os.getenv(\"FINAL_OLLAMA_MODEL\", os.getenv(\"OLLAMA_MODEL\", \"llama3\")),\n",
    "        \"hf\": os.getenv(\"FINAL_HF_MODEL\", os.getenv(\"HF_MODEL\", \"mistralai/Mistral-7B-Instruct-v0.2\"))\n",
    "    })\n",
    "    prompt = FINAL_GPT_PROMPT.format(mom_final_draft=mom_final_draft)\n",
    "    return llm_final.generate(prompt)\n",
    "\n",
    "# ------------------ LangGraph Orchestration ------------------\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "class MomState(TypedDict):\n",
    "    transcript: str\n",
    "    extracted: Dict[str, Any]\n",
    "    mom_draft: str\n",
    "    critique: Dict[str, Any]\n",
    "    mom_final: str\n",
    "    mom_final_polished: str\n",
    "\n",
    "def node_extract(state: MomState) -> MomState:\n",
    "    state[\"extracted\"] = extract_agent(state[\"transcript\"])\n",
    "    return state\n",
    "\n",
    "def node_summarize(state: MomState) -> MomState:\n",
    "    state[\"mom_draft\"] = summarize_agent(state[\"extracted\"])\n",
    "    return state\n",
    "\n",
    "def node_critique(state: MomState) -> MomState:\n",
    "    state[\"critique\"] = critique_agent(state[\"mom_draft\"])\n",
    "    return state\n",
    "\n",
    "def node_aggregate(state: MomState) -> MomState:\n",
    "    state[\"mom_final\"] = aggregate_agent(state[\"mom_draft\"], state[\"critique\"])\n",
    "    return state\n",
    "\n",
    "def node_polish(state: MomState) -> MomState:\n",
    "    state[\"mom_final_polished\"] = final_gpt_polish(state[\"mom_final\"])\n",
    "    return state\n",
    "\n",
    "workflow = StateGraph(MomState)\n",
    "workflow.add_node(\"extract\", node_extract)\n",
    "workflow.add_node(\"summarize\", node_summarize)\n",
    "workflow.add_node(\"critique\", node_critique)\n",
    "workflow.add_node(\"aggregate\", node_aggregate)\n",
    "workflow.add_node(\"polish\", node_polish)\n",
    "\n",
    "workflow.set_entry_point(\"extract\")\n",
    "workflow.add_edge(\"extract\", \"summarize\")\n",
    "workflow.add_edge(\"summarize\", \"critique\")\n",
    "workflow.add_edge(\"critique\", \"aggregate\")\n",
    "workflow.add_edge(\"aggregate\", \"polish\")\n",
    "workflow.add_edge(\"polish\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# ------------------ Export Helpers ------------------\n",
    "def save_mom_docx(mom_text: str, out_path: str) -> str:\n",
    "    doc = Document()\n",
    "    style = doc.styles['Normal']\n",
    "    style.font.name = 'Calibri'\n",
    "    style.font.size = Pt(11)\n",
    "    for line in mom_text.splitlines():\n",
    "        doc.add_paragraph(line)\n",
    "    doc.save(out_path)\n",
    "    return out_path\n",
    "\n",
    "def save_mom_pdf(mom_text: str, out_path: str) -> str:\n",
    "    c = canvas.Canvas(out_path, pagesize=A4)\n",
    "    width, height = A4\n",
    "    margin = 50\n",
    "    y = height - margin\n",
    "    for line in mom_text.splitlines():\n",
    "        if y < margin:\n",
    "            c.showPage()\n",
    "            y = height - margin\n",
    "        c.drawString(margin, y, line[:110])\n",
    "        y -= 14\n",
    "    c.save()\n",
    "    return out_path\n",
    "\n",
    "# ------------------ Run (Demo) ------------------\n",
    "# Set a path to your transcript (PDF/DOCX/TXT). If empty, a small demo text is used.\n",
    "path = \"\"  # e.g., \"/path/to/meeting_transcript.pdf\"\n",
    "\n",
    "if path:\n",
    "    transcript_text = ingest_any(path)\n",
    "else:\n",
    "    demo_text = [\n",
    "        \"Meeting Title: Q3 Launch Planning\",\n",
    "        \"Date/Time: 2025-08-10 15:00 IST\",\n",
    "        \"Participants: Alice (PM), Bob (Eng Lead), Carol (Design), Dev (Data)\",\n",
    "        \"Agenda: Scope, Risks, Timeline, Owners\",\n",
    "        \"Discussion:\",\n",
    "        \"  - Alice: Need final scope locked by Aug 25.\",\n",
    "        \"  - Bob: We can deliver API by Sep 10; risk on auth integration.\",\n",
    "        \"  - Carol: Assets by Aug 22; needs product sign-off.\",\n",
    "        \"  - Dev: Data pipeline ready; monitoring gaps remain.\",\n",
    "        \"Decisions:\",\n",
    "        \"  - Move feature X to Q3.1 patch.\",\n",
    "        \"Actions:\",\n",
    "        \"  - Bob to finalize auth POC by Aug 18.\",\n",
    "        \"  - Carol to share final design comps by Aug 21.\",\n",
    "        \"  - Dev to add monitoring alerts by Aug 20.\"\n",
    "    ]\n",
    "    transcript_text = \"\\n\".join(demo_text)\n",
    "\n",
    "state = {\n",
    "    \"transcript\": transcript_text,\n",
    "    \"extracted\": {},\n",
    "    \"mom_draft\": \"\",\n",
    "    \"critique\": {},\n",
    "    \"mom_final\": \"\",\n",
    "    \"mom_final_polished\": \"\"\n",
    "}\n",
    "\n",
    "result = app.invoke(state)\n",
    "final_text = result.get(\"mom_final_polished\") or result.get(\"mom_final\")\n",
    "\n",
    "os.makedirs(\"/mnt/data/output\", exist_ok=True)\n",
    "docx_file = save_mom_docx(final_text, \"/mnt/data/output/Minutes_of_Meeting_FINAL.docx\")\n",
    "pdf_file = save_mom_pdf(final_text, \"/mnt/data/output/Minutes_of_Meeting_FINAL.pdf\")\n",
    "print(\"Saved:\", docx_file, pdf_file)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}


What’s inside

Ingestion for PDF/DOCX/TXT (PyMuPDF, python-docx)

4-agent flow (Extract → Summarize → Critique → Aggregate)

New Step: Final GPT polish (configurable: OpenAI / Ollama / HF)

LangGraph orchestration with 5 nodes

Exports DOCX and PDF to /mnt/data/output/

Configure models (env vars)

LLM_BACKEND = ollama | hf | openai

OLLAMA_MODEL (e.g. llama3)

HF_MODEL (e.g. mistralai/Mistral-7B-Instruct-v0.2)

OPENAI_API_KEY, OPENAI_MODEL

LLM_FINAL_BACKEND = openai | ollama | hf | same

FINAL_MODEL (e.g. gpt-4o-mini)

Open the notebook, set the path to your transcript (or keep the demo), and run the last cell to generate:

/mnt/data/output/Minutes_of_Meeting_FINAL.docx

/mnt/data/output/Minutes_of_Meeting_FINAL.pdf
